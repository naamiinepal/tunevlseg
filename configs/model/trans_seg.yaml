_target_: src.models.image_text_mask_module.ImageTextMaskModule

net:
  _target_: src.models.core_models.trans_segmentor.TransformerSegmentor
  pretrained_model_name_or_path: ${model_pretrained_path}
  use_existing_proj: true
  freeze_encoders: false
  add_pos_enc: false
  transformer_decoder:
    _target_: torch.nn.TransformerDecoder
    decoder_layer:
      _target_: src.models.components.PreCrossAttentionTransformerDecoderLayer
      d_model: ${decoder_dim}
      nhead: 12
      dim_feedforward: 2048
      dropout: 0.1
      activation:
        _target_: torch.nn.GELU
        approximate: none
      batch_first: true
      norm_first: true
      bias: true
    num_layers: 4
    norm:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${decoder_dim}
  num_upsampler_layers: 5
  upsampler_act:
    _target_: torch.nn.ReLU
    inplace: true
  upsampler_norm: layer
  upsampler_num_channels_in_group: 64
  image_size: null
  num_output_channels: 1

loss_fn:
  _target_: monai.losses.DiceCELoss
  sigmoid: true
  lambda_dice: 1
  lambda_ce: 1

weight_decay: 0.0
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 2.0e-5

# lr_scheduler_config:
#   interval: step
#
# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   _partial_: true
#   T_max: 2.0e6
#   eta_min: 1.0e-6

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.2
  patience: 4

tokenizer_name_or_path: ${tokenizer_pretrained_path}

# compile model for faster training with pytorch 2.0
compile: false

task: binary
threshold: 0.5
