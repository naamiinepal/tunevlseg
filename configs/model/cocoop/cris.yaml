_target_: src.models.image_text_mask_module.ImageTextMaskModule

net:
  _target_: src.models.core_models.coop.COOPCRIS
  model_cfg:
    clip_pretrain: pretrain/RN50.pt
    fpn_in: [512, 1024, 1024]
    fpn_out: [256, 512, 1024]
    vis_dim: 512
    word_dim: 1024
    num_layers: 3
    num_head: 8
    dim_ffn: 2048
    dropout: 0.2
    return_intermediate: false
    img_size: ${img_size}
    freeze_encoder: true
    cris_pretrain: pretrain/cris_best_single.pth
  context_learner:
    _target_: src.models.core_models.coop.context_learner.CoCoOpContextLearner
    _partial_: true
    norm_image_features: false
    prompt_depth: 1
    use_unified_projection: false
    intermediate_dim: 64
    use_proj_norm: true
    use_lora_proj: false
    num_context: 4
    context_initializer: "a photo of a"
    vector_std: 0.02
    tokenizer:
      _target_: transformers.AutoTokenizer.from_pretrained
      pretrained_model_name_or_path: ${tokenizer_pretrained_path}
  freeze_all: true
  no_freeze_last_layer: false
  use_new_last_layer: true
  new_last_layer_kernel_size: 5
  residual_ratio: 0.5

loss_fn:
  _target_: monai.losses.DiceCELoss
  sigmoid: true
  lambda_dice: 1
  lambda_ce: 0.2

weight_decay: 0.0
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 2.0e-5

# lr_scheduler_config:
#   interval: step
#
# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   _partial_: true
#   T_max: 2.0e6
#   eta_min: 1.0e-6

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.2
  patience: 5

# compile model for faster training with pytorch 2.0
compile: false

task: binary
threshold: 0.5
