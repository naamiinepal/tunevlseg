_target_: src.models.image_text_mask_module.ImageTextMaskModule

net:
  _target_: src.models.core_models.coop.SharedAttnCLIPSeg
  model_cfg:
    pretrained_model_name_or_path: ${model_pretrained_path}
    freeze_encoder: false
    freeze_decoder: false
  context_learner:
    _target_: src.models.core_models.coop.context_learner.SharedAttnLearner
    _partial_: true
    prompt_depth: 1
    use_unified_projection: false
    num_context: 4
    vector_std: 0.02
    unified_projector:
      _target_: torch.nn.TransformerEncoderLayer
      _partial_: true
      nhead: 16
      dim_feedforward: 1536
      dropout: 0.25
  freeze_all: true
  no_freeze_last_layer: false
  use_new_last_layer: true
  new_last_layer_kernel_size: 5
  residual_ratio: 0.5

loss_fn:
  _target_: monai.losses.DiceCELoss
  sigmoid: true
  lambda_dice: 1
  lambda_ce: 0.2

weight_decay: 0.0
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 2.0e-4

# lr_scheduler_config:
#   interval: step
#
# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   _partial_: true
#   T_max: 2.0e6
#   eta_min: 1.0e-6

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.2
  patience: 5

# compile model for faster training with pytorch 2.0
compile: false

task: binary
threshold: 0.5
