# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: phrasecut
  - override /model: trans_seg
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb
  - override /extras: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["image_text_mask", "trans_segmentor", "phrasecut"]

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 1000
  gradient_clip_val: null
  accelerator: auto
  devices: [1]
  precision: 16-mixed
  val_check_interval: 0.5

model:
  weight_decay: 0.0
  optimizer:
    lr: 2.0e-5
  net:
    use_existing_proj: true
    freeze_encoders: true
    add_pos_enc: false
    transformer_decoder:
      decoder_layer:
        nhead: 16
        dropout: 0.1
        norm_first: true
      num_layers: 4
    num_upsampler_layers: 5
    image_size: ${img_size}
    output_bias: -1.748104048321891
  compile: false # Torch compile works only when CUDA Compat >= 7.0
  loss_fn:
    lambda_dice: 1
    lambda_ce: 0.2
    weight:
      _target_: torch.tensor
      data: 5.8

# The dimension after projection
decoder_dim: 512

data:
  batch_size: 64
  drop_last: true
  train_ds:
    neg_prob: 0.1

logger:
  wandb:
    project: Transformer Segmentor Normalized
    name: ${exp_name}
    tags: ${tags}
    group: "phrasecut"
  aim:
    experiment: "phrasecut"

############################
# Additional parameters    #
############################

# model_pretrained_path: openai/clip-vit-base-patch16
model_pretrained_path: google/siglip-base-patch16-384
tokenizer_pretrained_path: ${model_pretrained_path}

exp_name: "img_${img_size}_b${data.batch_size}_lr${model.optimizer.lr}_p${trainer.precision}"

# img_size: 352
img_size: 384
data_root: ${oc.env:PHRASECUT_ROOT}

img_mean: [0.48145466, 0.4578275, 0.40821073]
img_std: [0.26862954, 0.26130258, 0.27577711]

# Image pre-processing configs
train_transforms:
  _target_: albumentations.Compose
  transforms:
    - _target_: albumentations.Resize
      height: ${img_size}
      width: ${img_size}
      interpolation: ${import_eval:cv2.INTER_CUBIC}
    - _target_: albumentations.Rotate
      limit: 10
      interpolation: ${import_eval:cv2.INTER_CUBIC}
      border_mode: ${import_eval:cv2.BORDER_REPLICATE}
      p: 0.2
    - _target_: albumentations.RandomScale
      scale_limit: 0.1
      interpolation: ${import_eval:cv2.INTER_CUBIC}
      p: 0.2
    - _target_: albumentations.PadIfNeeded
      min_height: ${img_size}
      min_width: ${img_size}
      border_mode: ${import_eval:cv2.BORDER_REPLICATE}
    - _target_: albumentations.CropNonEmptyMaskIfExists
      width: ${img_size}
      height: ${img_size}
    - _target_: albumentations.RandomBrightnessContrast
      contrast_limit: 0.1
      brightness_limit: 0.1
      brightness_by_max: false
      p: 0.2
    - _target_: albumentations.Normalize
      mean: ${img_mean}
      std: ${img_std}
    - _target_: albumentations.pytorch.ToTensorV2
      transpose_mask: true

_eval_transforms:
  _target_: albumentations.Compose
  transforms:
    - _target_: albumentations.Resize
      height: ${img_size}
      width: ${img_size}
      interpolation: ${import_eval:cv2.INTER_CUBIC}
    - _target_: albumentations.Normalize
      mean: ${img_mean}
      std: ${img_std}
    - _target_: albumentations.pytorch.ToTensorV2
      transpose_mask: true

val_transforms: ${_eval_transforms}
test_transforms: ${_eval_transforms}

collate_fn:
  _target_: src.data.components.data_collator.CustomDataCollatorWithPadding
  tokenizer:
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: ${tokenizer_pretrained_path}
  padding_keys: ["input_ids", "attention_mask"]
  padding: true
  max_length: null
  pad_to_multiple_of: null
  return_tensors: pt
