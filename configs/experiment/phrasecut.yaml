# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: phrasecut
  - override /model: trans_seg
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb
  - override /extras: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["image_text_mask", "trans_segmentor", "phrasecut"]

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 1000
  gradient_clip_val: null
  accelerator: auto
  devices: [1]
  precision: 16-mixed
  val_check_interval: 0.5

model:
  weight_decay: 0.0
  optimizer:
    lr: 2.0e-4
  net:
    freeze_image_encoder: true
    freeze_text_encoder: true
    add_pos_enc: false
    decoder_layer_kwargs:
      nhead: 8
      dropout: 0.1
      norm_first: true
    num_decoder_layers: 4
    num_upsampler_layers: 5
    image_size: ${img_size}
  compile: false # Torch compile works only when CUDA Compat >= 7.0

data:
  batch_size: 64
  drop_last: true

logger:
  wandb:
    project: Transformer Segmentor
    name: ${exp_name}
    tags: ${tags}
    group: "phrasecut"
  aim:
    experiment: "phrasecut"

############################
# Additional parameters    #
############################

model_pretrained_path: openai/clip-vit-base-patch16
tokenizer_pretrained_path: ${model_pretrained_path}
image_pretrained_path: ${model_pretrained_path}
text_pretrained_path: ${model_pretrained_path}

exp_name: "img_${img_size}_b${data.batch_size}_lr${model.optimizer.lr}_p${trainer.precision}"
output_masks_dir: "output_masks/phrasecut/${exp_name}"

img_size: 224
data_root: ${oc.env:PHRASECUT_ROOT}

img_mean: [0.48145466, 0.4578275, 0.40821073]
img_std: [0.26862954, 0.26130258, 0.27577711] 

# Image pre-processing configs
train_transforms:
  _target_: albumentations.Compose
  transforms:
    - _target_: albumentations.SmallestMaxSize
      max_size: ${img_size}
    - _target_: albumentations.Rotate
      limit: 10
      border_mode: ${import_eval:cv2.BORDER_REPLICATE}
      p: 0.2
    - _target_: albumentations.RandomCrop
      width: ${img_size}
      height: ${img_size}
    - _target_: albumentations.RandomBrightnessContrast
      contrast_limit: 0.1
      brightness_limit: 0.1
      brightness_by_max: false
      p: 0.2
    - _target_: albumentations.Normalize
      mean: ${img_mean}
      std: ${img_std}
    - _target_: albumentations.pytorch.ToTensorV2
      transpose_mask: true

_eval_transforms: 
  _target_: albumentations.Compose
  transforms:
    - _target_: albumentations.Resize
      height: ${img_size}
      width: ${img_size}
    - _target_: albumentations.Normalize
      mean: ${img_mean}
      std: ${img_std}
    - _target_: albumentations.pytorch.ToTensorV2
      transpose_mask: true

val_transforms: ${_eval_transforms}
test_transforms: ${_eval_transforms}
