{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368bd63d-f3e3-4ad8-adee-bbfa69c8666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720c071e-2f47-456d-b7b3-3f98870d2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import hydra\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from src.models.core_models.zero_shot_ris import ZeroShotRIS\n",
    "from src.data.core_datasets.phrasecutdataset import PhraseCutDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1315efc1-98ac-42e6-9b50-2b8993a49ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CUDNN_BENCHMARK': False, 'DATALOADER': {'ASPECT_RATIO_GROUPING': True, 'FILTER_EMPTY_ANNOTATIONS': True, 'NUM_WORKERS': 4, 'REPEAT_THRESHOLD': 0.0, 'SAMPLER_TRAIN': 'TrainingSampler'}, 'DATASETS': {'PRECOMPUTED_PROPOSAL_TOPK_TEST': 1000, 'PRECOMPUTED_PROPOSAL_TOPK_TRAIN': 2000, 'PROPOSAL_FILES_TEST': [], 'PROPOSAL_FILES_TRAIN': [], 'TEST': ['coco_2017_val'], 'TRAIN': ['coco_2017_train_unlabeled_densecl_r101']}, 'GLOBAL': {'HACK': 1.0}, 'INPUT': {'CROP': {'ENABLED': False, 'SIZE': [0.9, 0.9], 'TYPE': 'relative_range'}, 'FORMAT': 'RGB', 'MASK_FORMAT': 'bitmask', 'MAX_SIZE_TEST': 1333, 'MAX_SIZE_TRAIN': 1333, 'MIN_SIZE_TEST': 800, 'MIN_SIZE_TRAIN': [640, 672, 704, 736, 768, 800], 'MIN_SIZE_TRAIN_SAMPLING': 'choice', 'RANDOM_FLIP': 'horizontal'}, 'MODEL': {'ANCHOR_GENERATOR': {'ANGLES': [[-90, 0, 90]], 'ASPECT_RATIOS': [[0.5, 1.0, 2.0]], 'NAME': 'DefaultAnchorGenerator', 'OFFSET': 0.0, 'SIZES': [[32, 64, 128, 256, 512]]}, 'BACKBONE': {'FREEZE_AT': 0, 'NAME': 'build_resnet_fpn_backbone'}, 'FPN': {'FUSE_TYPE': 'sum', 'IN_FEATURES': ['res2', 'res3', 'res4', 'res5'], 'NORM': '', 'OUT_CHANNELS': 256}, 'KEYPOINT_ON': False, 'LOAD_PROPOSALS': False, 'MASK_ON': True, 'META_ARCHITECTURE': 'PseudoSOLOv2', 'PANOPTIC_FPN': {'COMBINE': {'ENABLED': True, 'INSTANCES_CONFIDENCE_THRESH': 0.5, 'OVERLAP_THRESH': 0.5, 'STUFF_AREA_LIMIT': 4096}, 'INSTANCE_LOSS_WEIGHT': 1.0}, 'PIXEL_MEAN': [123.675, 116.28, 103.53], 'PIXEL_STD': [58.395, 57.12, 57.375], 'PROPOSAL_GENERATOR': {'MIN_SIZE': 0, 'NAME': 'RPN'}, 'RESNETS': {'DEFORM_MODULATED': False, 'DEFORM_NUM_GROUPS': 1, 'DEFORM_ON_PER_STAGE': [False, False, False, False], 'DEPTH': 101, 'NORM': 'FrozenBN', 'NUM_GROUPS': 1, 'OUT_FEATURES': ['res2', 'res3', 'res4', 'res5'], 'RES2_OUT_CHANNELS': 256, 'RES5_DILATION': 1, 'STEM_OUT_CHANNELS': 64, 'STRIDE_IN_1X1': False, 'WIDTH_PER_GROUP': 64}, 'RETINANET': {'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_WEIGHTS': [1.0, 1.0, 1.0, 1.0], 'FOCAL_LOSS_ALPHA': 0.25, 'FOCAL_LOSS_GAMMA': 2.0, 'IN_FEATURES': ['p3', 'p4', 'p5', 'p6', 'p7'], 'IOU_LABELS': [0, -1, 1], 'IOU_THRESHOLDS': [0.4, 0.5], 'NMS_THRESH_TEST': 0.5, 'NORM': '', 'NUM_CLASSES': 80, 'NUM_CONVS': 4, 'PRIOR_PROB': 0.01, 'SCORE_THRESH_TEST': 0.05, 'SMOOTH_L1_LOSS_BETA': 0.1, 'TOPK_CANDIDATES_TEST': 1000}, 'ROI_BOX_CASCADE_HEAD': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0], [30.0, 30.0, 15.0, 15.0]], 'IOUS': [0.5, 0.6, 0.7]}, 'ROI_BOX_HEAD': {'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_LOSS_WEIGHT': 1.0, 'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0], 'CLS_AGNOSTIC_BBOX_REG': False, 'CONV_DIM': 256, 'FC_DIM': 1024, 'NAME': '', 'NORM': '', 'NUM_CONV': 0, 'NUM_FC': 0, 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'POOLER_TYPE': 'ROIAlignV2', 'SMOOTH_L1_BETA': 0.0, 'TRAIN_ON_PRED_BOXES': False}, 'ROI_HEADS': {'BATCH_SIZE_PER_IMAGE': 512, 'IN_FEATURES': ['res4'], 'IOU_LABELS': [0, 1], 'IOU_THRESHOLDS': [0.5], 'NAME': 'Res5ROIHeads', 'NMS_THRESH_TEST': 0.5, 'NUM_CLASSES': 80, 'POSITIVE_FRACTION': 0.25, 'PROPOSAL_APPEND_GT': True, 'SCORE_THRESH_TEST': 0.05}, 'ROI_KEYPOINT_HEAD': {'CONV_DIMS': [512, 512, 512, 512, 512, 512, 512, 512], 'LOSS_WEIGHT': 1.0, 'MIN_KEYPOINTS_PER_IMAGE': 1, 'NAME': 'KRCNNConvDeconvUpsampleHead', 'NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS': True, 'NUM_KEYPOINTS': 17, 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'POOLER_TYPE': 'ROIAlignV2'}, 'ROI_MASK_HEAD': {'CLS_AGNOSTIC_MASK': False, 'CONV_DIM': 256, 'NAME': 'MaskRCNNConvUpsampleHead', 'NORM': '', 'NUM_CONV': 0, 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'POOLER_TYPE': 'ROIAlignV2'}, 'RPN': {'BATCH_SIZE_PER_IMAGE': 256, 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_LOSS_WEIGHT': 1.0, 'BBOX_REG_WEIGHTS': [1.0, 1.0, 1.0, 1.0], 'BOUNDARY_THRESH': -1, 'CONV_DIMS': [-1], 'HEAD_NAME': 'StandardRPNHead', 'IN_FEATURES': ['res4'], 'IOU_LABELS': [0, -1, 1], 'IOU_THRESHOLDS': [0.3, 0.7], 'LOSS_WEIGHT': 1.0, 'NMS_THRESH': 0.7, 'POSITIVE_FRACTION': 0.5, 'POST_NMS_TOPK_TEST': 1000, 'POST_NMS_TOPK_TRAIN': 2000, 'PRE_NMS_TOPK_TEST': 6000, 'PRE_NMS_TOPK_TRAIN': 12000, 'SMOOTH_L1_BETA': 0.0}, 'SEM_SEG_HEAD': {'COMMON_STRIDE': 4, 'CONVS_DIM': 128, 'IGNORE_VALUE': 255, 'IN_FEATURES': ['p2', 'p3', 'p4', 'p5'], 'LOSS_WEIGHT': 1.0, 'NAME': 'SemSegFPNHead', 'NORM': 'GN', 'NUM_CLASSES': 54}, 'SOLOV2': {'FPN_INSTANCE_STRIDES': [8, 8, 16, 32, 32], 'FPN_SCALE_RANGES': [[1, 96], [48, 192], [96, 384], [192, 768], [384, 2048]], 'FREEZE': False, 'INSTANCE_CHANNELS': 512, 'INSTANCE_IN_CHANNELS': 256, 'INSTANCE_IN_FEATURES': ['p2', 'p3', 'p4', 'p5', 'p6'], 'IS_FREEMASK': False, 'LOSS': {'DICE_WEIGHT': 1.0, 'FOCAL_ALPHA': 0.25, 'FOCAL_GAMMA': 2.0, 'FOCAL_USE_SIGMOID': True, 'FOCAL_WEIGHT': 1.0}, 'MASK_CHANNELS': 128, 'MASK_IN_CHANNELS': 256, 'MASK_IN_FEATURES': ['p2', 'p3', 'p4', 'p5'], 'MASK_THR': 0.5, 'MAX_PER_IMG': 100, 'NMS_KERNEL': 'gaussian', 'NMS_PRE': 500, 'NMS_SIGMA': 2, 'NMS_TYPE': 'matrix', 'NORM': 'GN', 'NUM_CLASSES': 2, 'NUM_GRIDS': [40, 36, 24, 16, 12], 'NUM_INSTANCE_CONVS': 4, 'NUM_KERNELS': 256, 'NUM_MASKS': 256, 'PRIOR_PROB': 0.01, 'SCORE_THR': 0.1, 'SIGMA': 0.2, 'TYPE_DCN': 'DCN', 'UPDATE_THR': 0.05, 'USE_COORD_CONV': True, 'USE_DCN_IN_INSTANCE': False}, 'WEIGHTS': 'checkpoints/FreeSOLO_R101_30k_pl.pth'}, 'OUTPUT_DIR': 'training_dir/FreeSOLO_pl', 'SEED': -1, 'SOLVER': {'AMP': {'ENABLED': False}, 'BASE_LR': 0.001, 'BIAS_LR_FACTOR': 1.0, 'CHECKPOINT_PERIOD': 5000, 'CLIP_GRADIENTS': {'CLIP_TYPE': 'value', 'CLIP_VALUE': 1.0, 'ENABLED': False, 'NORM_TYPE': 2.0}, 'GAMMA': 0.1, 'IMS_PER_BATCH': 16, 'LR_SCHEDULER_NAME': 'WarmupMultiStepLR', 'MAX_ITER': 30000, 'MOMENTUM': 0.9, 'NESTEROV': False, 'REFERENCE_WORLD_SIZE': 0, 'STEPS': [], 'WARMUP_FACTOR': 0.01, 'WARMUP_ITERS': 1000, 'WARMUP_METHOD': 'linear', 'WEIGHT_DECAY': 0.0001, 'WEIGHT_DECAY_BIAS': None, 'WEIGHT_DECAY_NORM': 0.0}, 'TEST': {'AUG': {'ENABLED': False, 'FLIP': True, 'MAX_SIZE': 4000, 'MIN_SIZES': [400, 500, 600, 700, 800, 900, 1000, 1100, 1200]}, 'DETECTIONS_PER_IMAGE': 100, 'EVAL_PERIOD': 0, 'EXPECTED_RESULTS': [], 'KEYPOINT_OKS_SIGMAS': [], 'PRECISE_BN': {'ENABLED': False, 'NUM_ITER': 200}}, 'VERSION': 2, 'VIS_PERIOD': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with hydra.initialize(version_base=\"1.3\", config_path=\"../conf\"):\n",
    "    cfg = hydra.compose(config_name=\"config.yaml\")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c063f41d-6397-43a7-8069-ed0e9246db24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mZeroShotRIS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_pretrained_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolo_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolo_state_dict_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Projects/ml-scratchpad/src/models/core_models/zero_shot_ris/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ZeroShotRIS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5000bc3-7718-4e10-b537-bdb6705c703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = \"openai/clip-vit-base-patch32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ba913d-c3c9-4aa0-b245-ae57e3f89ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASECUT_ROOT=\"/run/media/maverick/Backup/datasets/phrasecut\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ceefdb1-d42a-44de-956d-1a22d2abe818",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "img_mean= [0.48145466, 0.4578275, 0.40821073]\n",
    "img_std= [0.26862954, 0.26130258, 0.27577711] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7482c79e-3291-4601-a3b5-505340f7de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([    \n",
    "    # A.Resize(height=img_size, width=img_size, interpolation=cv2.INTER_CUBIC),\n",
    "    A.Normalize(mean=img_mean,std=img_std),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460d8570-b721-427a-9f9f-96804ff9ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PhraseCutDataset(\n",
    "    data_root=PHRASECUT_ROOT,\n",
    "    task_json_path=\"filtered_tasks/refer_train.json\",\n",
    "    tokenizer_pretrained_path=pretrained_path,\n",
    "    transforms=transforms,\n",
    "    return_tensors=\"pt\",\n",
    "    prompt_method=\"fixed\",\n",
    "    neg_prob=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4571ae1d-0bfb-446d-b902-896bd2e5c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de9af8de-daf7-47b5-8ba5-afd4892ea19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[[-0.2594, -0.5660, -0.8580,  ..., -0.0405, -0.2448, -0.0550],\n",
       "           [-0.2448, -0.5222, -0.7850,  ..., -0.1280, -0.2740, -0.1280],\n",
       "           [-0.2594, -0.4930, -0.7412,  ..., -0.6974, -0.2886, -0.0842],\n",
       "           ...,\n",
       "           [-0.6244, -0.5806, -0.6098,  ..., -0.2302, -0.2156, -0.2302],\n",
       "           [-0.6244, -0.5952, -0.6390,  ..., -0.2156, -0.2010, -0.2010],\n",
       "           [-0.6098, -0.6098, -0.6682,  ..., -0.1280, -0.1134, -0.0842]],\n",
       " \n",
       "          [[-0.7166, -0.9717, -1.1818,  ..., -1.3169, -1.0017, -0.8967],\n",
       "           [-0.7016, -0.9267, -1.1218,  ..., -0.9867, -1.0767, -1.3319],\n",
       "           [-0.7166, -0.8967, -1.0767,  ..., -1.3769, -0.8066, -0.5365],\n",
       "           ...,\n",
       "           [-1.0317, -0.9867, -1.0167,  ..., -0.7916, -0.7766, -0.7916],\n",
       "           [-1.0317, -1.0017, -1.0467,  ..., -0.7766, -0.7616, -0.7616],\n",
       "           [-1.0167, -1.0167, -1.0767,  ..., -0.6865, -0.6715, -0.6415]],\n",
       " \n",
       "          [[-0.8688, -1.0394, -1.1816,  ..., -1.4518, -1.2385, -0.9967],\n",
       "           [-0.8545, -1.0252, -1.1247,  ..., -1.2243, -1.2954, -1.3238],\n",
       "           [-0.8688, -0.9967, -1.1105,  ..., -1.4802, -0.9541, -0.6697],\n",
       "           ...,\n",
       "           [-1.1247, -1.0821, -1.1105,  ..., -1.0536, -1.0394, -1.0536],\n",
       "           [-1.1247, -1.0678, -1.1105,  ..., -1.0394, -1.0252, -1.0252],\n",
       "           [-1.1105, -1.0821, -1.1389,  ..., -0.9541, -0.9399, -0.9114]]]]),\n",
       " 'mask': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " 'mask_shape': tensor([[300, 500]]),\n",
       " 'mask_name': ['2359296__3539024-windows.png'],\n",
       " 'input_ids': tensor([[49406,   320,  1125,   539,  5437,   269, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(dl))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "745a9316-42b9-4417-8c36-75c2d9f94576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 300, 500])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c3f922-2079-4a12-adc8-6481b062ce69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153fea98-37c0-4516-a460-8675c9f7be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = sample[\"image\"]\n",
    "text_input = {\n",
    "    \"input_ids\": sample[\"input_ids\"].expand(2, -1),\n",
    "    \"attention_mask\": sample[\"attention_mask\"].expand(2, -1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc26bf5f-0077-4c3b-8b8f-5471b1f2e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsmodel = ZeroShotRIS(pretrained_path, cfg, \"/run/media/maverick/Backup/checkpoints/FreeSOLO_R101_30k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2fb9e07-0121-432e-b836-eab2f1bb8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maverick/Projects/ml-scratchpad/.venv/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Mask Shape: torch.Size([33, 300, 500])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (500) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m zsmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 3\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mzsmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-scratchpad/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-scratchpad/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ml-scratchpad/notebooks/../src/models/core_models/zero_shot_ris/__init__.py:133\u001b[0m, in \u001b[0;36mZeroShotRIS.forward\u001b[0;34m(self, image_input, text_input)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pred masks given. No image features are returned.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    128\u001b[0m         (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mimage_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]),\n\u001b[1;32m    129\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mimage_input\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    130\u001b[0m         device\u001b[38;5;241m=\u001b[39mimage_input\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    131\u001b[0m     )\n\u001b[0;32m--> 133\u001b[0m visual_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_visual_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m text_ensemble \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_text_ensemble(text_input)\n\u001b[1;32m    137\u001b[0m max_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_max_index(text_ensemble, visual_feature)\n",
      "File \u001b[0;32m~/Projects/ml-scratchpad/notebooks/../src/models/core_models/zero_shot_ris/__init__.py:104\u001b[0m, in \u001b[0;36mZeroShotRIS.get_visual_feature\u001b[0;34m(self, image_input, pred_boxes, pred_masks)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction Mask Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_masks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    102\u001b[0m mask_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mask_features(image_input, pred_masks)\n\u001b[0;32m--> 104\u001b[0m crop_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cropped_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m mask_features \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha) \u001b[38;5;241m*\u001b[39m crop_features\n",
      "File \u001b[0;32m~/Projects/ml-scratchpad/notebooks/../src/models/core_models/zero_shot_ris/__init__.py:30\u001b[0m, in \u001b[0;36mZeroShotRIS.get_cropped_features\u001b[0;34m(self, image_input, pred_boxes, pred_masks)\u001b[0m\n\u001b[1;32m     26\u001b[0m cropped_imgs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred_box, pred_mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred_boxes, pred_masks, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# pred_mask, pred_box = pred_mask.type(torch.uint8), pred_box.type(torch.int)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     masked_image \u001b[38;5;241m=\u001b[39m image_input \u001b[38;5;241m*\u001b[39m pred_mask \u001b[38;5;241m+\u001b[39m \u001b[38;5;241;43m~\u001b[39;49m\u001b[43mpred_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpixel_mean\u001b[49m\n\u001b[1;32m     32\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mint\u001b[39m(pred_box[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mint\u001b[39m(pred_box[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mint\u001b[39m(pred_box[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mint\u001b[39m(pred_box[\u001b[38;5;241m3\u001b[39m]),\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     masked_image \u001b[38;5;241m=\u001b[39m TF\u001b[38;5;241m.\u001b[39mresized_crop(\n\u001b[1;32m     40\u001b[0m         masked_image,\n\u001b[1;32m     41\u001b[0m         y1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m         antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (500) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "zsmodel.eval()\n",
    "with torch.inference_mode():\n",
    "    output = zsmodel(image_input, text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1df111-9c04-447a-8a89-1af097b7a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4dfaa-4278-4cee-ace8-9b8f250adea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdaaa19-9f24-43ca-b027-3451d9e48570",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_image = sample[\"image\"][0].movedim(0, -1).numpy()\n",
    "\n",
    "img_mean= np.array([0.48145466, 0.4578275, 0.40821073])\n",
    "img_std= np.array([0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "norm_image = np_image * img_std + img_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013f223-6bb4-4e89-a094-4cdce29c6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax1.imshow(norm_image)\n",
    "ax2.imshow(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020baac9-0f86-4e08-9a83-557888f37ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_img(img):\n",
    "    img_min = img.min((0, 1))\n",
    "    return (img - img_min) / (img.max((0, 1)) - img_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4eca9-6d0b-4a63-af0e-f7331d6f9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample[\"mask\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9d783-367a-4512-8681-eef7f9b39f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
