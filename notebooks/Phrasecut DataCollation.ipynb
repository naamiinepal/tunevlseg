{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb7d04c-c152-4583-944c-9779de3d7fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maverick/Projects/ml-scratchpad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maverick/Projects/ml-scratchpad/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2320c184-f265-4d3c-8cd0-46953bfc73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls src/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63a6b224-ce7f-4b38-854d-3f0b977b9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from transformers import DataCollatorWithPadding\n",
    "from src.data.core_datasets import PhraseCutDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a22c3528-a708-4e3f-964d-3b5a736dbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PhraseCutDataset(\n",
    "    data_root=\"/run/media/maverick/Backup/datasets/phrasecut\",\n",
    "    task_json_path=\"refer_train.json\",\n",
    "    tokenizer_pretrained_path=\"openai/clip-vit-base-patch32\",\n",
    "    prompt_method=\"shuffle+\",\n",
    "    neg_prob=0.2,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "472d991f-898e-4b94-a755-699b93968c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataCollatorWithPadding(DataCollatorWithPadding):\n",
    "    def __init__(self, padding_keys, *args, **kwargs):\n",
    "        self.padding_keys = set(padding_keys)\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        first_example = batch[0]\n",
    "        padding_features = [\n",
    "            {key: value for key, value in example.items() if key in self.padding_keys}\n",
    "            for example in batch\n",
    "        ]\n",
    "\n",
    "        padded_features = super().__call__(padding_features)\n",
    "\n",
    "        no_padding_features = [\n",
    "            {key: value for key, value in example.items() if key not in padded_features}\n",
    "            for example in batch\n",
    "        ]\n",
    "\n",
    "        collated_features = default_collate(no_padding_features)\n",
    "        \n",
    "        return {**collated_features, **padded_features}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e143e010-1deb-4432-a029-60e04509e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, num_workers=0, batch_size=1, collate_fn=CustomDataCollatorWithPadding(padding_keys=[\"input_ids\", \"attention_mask\"], tokenizer=ds.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d62b4d6-a7a5-44f6-90d9-f821fa10fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__class_getitem__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_encodings', '_n_sequences', 'char_to_token', 'char_to_word', 'clear', 'convert_to_tensors', 'copy', 'data', 'encodings', 'fromkeys', 'get', 'is_fast', 'items', 'keys', 'n_sequences', 'pop', 'popitem', 'sequence_ids', 'setdefault', 'to', 'token_to_chars', 'token_to_sequence', 'token_to_word', 'tokens', 'update', 'values', 'word_ids', 'word_to_chars', 'word_to_tokens', 'words']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[[105,  69,  43],\n",
       "           [ 84,  52,  31],\n",
       "           [ 64,  38,  21],\n",
       "           ...,\n",
       "           [120,  29,   2],\n",
       "           [106,  50,  17],\n",
       "           [119,  57,  34]],\n",
       " \n",
       "          [[106,  70,  44],\n",
       "           [ 87,  55,  32],\n",
       "           [ 69,  42,  25],\n",
       "           ...,\n",
       "           [114,  51,  18],\n",
       "           [104,  45,  13],\n",
       "           [114,  28,  11]],\n",
       " \n",
       "          [[105,  69,  43],\n",
       "           [ 89,  57,  34],\n",
       "           [ 72,  45,  26],\n",
       "           ...,\n",
       "           [ 75,  25,   0],\n",
       "           [103,  63,  37],\n",
       "           [117,  81,  57]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 80,  48,  25],\n",
       "           [ 83,  51,  28],\n",
       "           [ 81,  49,  26],\n",
       "           ...,\n",
       "           [107,  64,  30],\n",
       "           [108,  65,  31],\n",
       "           [107,  64,  30]],\n",
       " \n",
       "          [[ 80,  48,  25],\n",
       "           [ 82,  50,  29],\n",
       "           [ 79,  47,  26],\n",
       "           ...,\n",
       "           [108,  65,  31],\n",
       "           [109,  66,  32],\n",
       "           [109,  66,  32]],\n",
       " \n",
       "          [[ 81,  49,  26],\n",
       "           [ 81,  49,  28],\n",
       "           [ 77,  45,  24],\n",
       "           ...,\n",
       "           [114,  71,  37],\n",
       "           [115,  72,  38],\n",
       "           [117,  74,  40]]]], dtype=torch.uint8),\n",
       " 'mask': tensor([[[[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]],\n",
       " \n",
       "          [[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           ...,\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]]]),\n",
       " 'mask_shape': tensor([[300, 500]]),\n",
       " 'mask_name': ['2359296__3539024-windows.png'],\n",
       " 'input_ids': tensor([[49406,   320, 31139,  8853,   539,  4381,   269, 49407]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = next(iter(dl))\n",
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e71bcf2c-7147-4bce-9661-7fcbab76cc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   320,  1125,   539,  4287,  2912,  3814,   269, 49407]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e17a1-fb52-48b1-9ff1-14d866e2bef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
